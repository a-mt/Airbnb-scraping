{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness modeling: a hands-on introduction\n",
    "This notebook will introduce you to fairlearn. Fairlearn is a fairness modelling tool that is developed and maintained by Microsoft. The tool allows the user to assess whether a machine learning model is fair. It also implemented methods to solve unfairness. We use fairlearn and not another tool for a couple of reasons:\n",
    "- The tool is free to use: it is an open-source package that can be used with python;\n",
    "- The tool is complete: it can be used to assess unfairness and also to solve the problem of unfairness;\n",
    "- The tool has a very nice visual interface for Jupyter notebooks which makes it very easy to learn;\n",
    "- The tool is used in industry by for example EY;\n",
    "- The tool has excellent documentation compared to the other tools that are available.\n",
    "\n",
    "## Data\n",
    "In order to demonstrate fairlearn, the [adult income](https://www.kaggle.com/wenruliu/adult-income-dataset) dataset will be used. This is a very commonly used dataset. Usually the target variable of the dataset is the income variable which denotes whether a person makes more or less than 50k per year. We will however consider a different target variable. The target variable in our case denotes whether or not a person paid back a loan. The original dataset can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"adult.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the new target variable in a very simple way. In case the person made more than 50K, then we assume they paid back the loan. Otherwise, they did not. We also drop the income variable from the dataset because it can perfectly predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"target\"] = df.income.apply(lambda x: int(x == \">50K\")) # int(): Fairlearn dashboard requires integer target\n",
    "df = df.drop(\"income\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model\n",
    "The dataframe contains over 48 000 observations. For the sake of simplicity, we will not do any variable selection and just include all the variables in the dataframe. **We will also not split the data into a train and test dataset since this is just for the purpose of demonstration**. There is however one step that we need to take. Some of the variables in our data (i.e. **race** and **gender**) can be considered as sensitive or protected attributes. It is a best practice to not include these variables in the dataset because you do not want the model to take them into account. We will however save them in a new variable because they will be needed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = df.pop(\"race\") # Pop function drops and assigns at the same time\n",
    "gender = df.pop(\"gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the variables in a model, we need to transform the categorical columns into dummy variables. Before we do that, we will take away the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop(\"target\")\n",
    "df = pd.get_dummies(df, drop_first = False) # decision tree will be used as the model -> don't drop first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demonstration, I will be using a simple decision tree classifier and use it to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4) # parameters have not been tuned\n",
    "classifier.fit(df, target)\n",
    "\n",
    "# Note that we are predicting using the same data as we used for training, this is just for the sake of example\n",
    "# Never do this in real life\n",
    "prediction = classifier.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to properly assess the performance of the model, we need to know the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.760718\n",
       "1    0.239282\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fairlearn to assess the unfairness\n",
    "In order to use fairlearn, we first have install the fairlearn library as well as the ipywidgets library. You can install them using the command prompt and the commands below (run them one by one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fairlearn\n",
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we can make use of the fairlearn package. The first way is by using functions that are defined in the package. The second way is by using the visual interface. First, we will focus on using the functions.\n",
    "\n",
    "### Identifying fairness\n",
    "The functions in the fairlearn package that can be used to identify unfairness mostly make use of the sklearn metrics. The most commonly used metrics ar:\n",
    "- **Accuracy_score**: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true;\n",
    "- **AUC**: computes Area Under the Curve (AUC of the ROC using the trapezoidal rule;\n",
    "- **Balanced_accuracy_score**: it is defined as the average of recall obtained on each class;\n",
    "- **F1_score**: the F1 score can be interpreted as a weighted average of the precision and recall;\n",
    "- **Recall_score**:  the recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "- **Precision_score**: the precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "We can calculate these metrics for every group that falls under a protected variable. By example, we can compute the accuracy for males and females separately. In fairlean, we do this by calling the *group_summary* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 0.8443552680070431,\n",
       " 'by_group': {'Female': 0.9251482213438735, 'Male': 0.8042879019908117}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.metrics import group_summary\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "group_summary(accuracy_score , target, prediction, sensitive_features = gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides being able to use the sklearn metrics, fairlearn also allows the user to calculate specialized metrics for fairness like demographic parity, equalized odds etc. The following metrics are available as part of *fairlearn.metrics*:\n",
    "- **demographic_parity_difference**: the difference between the largest and the smallest group-level selection rate across all values of the sensitive parameter. A value of 0 means that all groups have the same selection rate;\n",
    "- **demographic_parity_ratio**: the ratio instead of the difference.\n",
    "- **difference_from_summary**: the difference between the maximum and the minimum metric value across groups (using the sklearn metrics);\n",
    "- **equalized_odds_difference**: the smaller of two metrics, the true positive rate and the false positive rate. An equalized odds difference of 0 means that all groups have the same true positive, true negative, false positive and false negative rate;\n",
    "- **equalized_odds_ratio**: the ratio instead of the difference;\n",
    "- **false_positive_rate**: self explanatory;\n",
    "- **false_negative_rate**: self explanatory;\n",
    "- **true_positive_rate**: self explanatory;\n",
    "- **true_negative_rate**: self explanatory.\n",
    "\n",
    "A complete list can be found [here](https://fairlearn.github.io/api_reference/fairlearn.metrics.html). In order to make a decision of whether the model is fair, we need to define cut-off values for these metrics. Microsoft has not specified these cut-off values in the fairlearn tool. Luckily, IBM did specify them in their Fairness 360 tool.\n",
    "\n",
    "The cut-off values for the different metrics are:\n",
    "- Demograpic parity difference: if the absolute value is smaller than 0.1, the model can be considered fair\n",
    "- Equalized odds difference: if the absolute value is smaller than 0.1, the model can be considered fair\n",
    "- Equal opportunity difference:  if the absolute value is smaller than 0.1, the model can be considered fair\n",
    "- Demographic parity ratio: fairness for this metric is between 0.8 and 1.25\n",
    "\n",
    "As an example, let's calculate the demographic parity difference, followed by the equalized odds difference for the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.15\n",
      "Equalized odds difference: 0.08\n",
      "Demographic parity ratio: 0.3\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, demographic_parity_ratio\n",
    "\n",
    "dpd = demographic_parity_difference(target, prediction, sensitive_features = gender)\n",
    "eod = equalized_odds_difference(target, prediction, sensitive_features = gender)\n",
    "dpr = demographic_parity_ratio(target, prediction, sensitive_features = gender)\n",
    "\n",
    "print(\"Demographic parity difference: {}\".format(round(dpd, 2)))\n",
    "print(\"Equalized odds difference: {}\".format(round(eod, 2)))\n",
    "print(\"Demographic parity ratio: {}\".format(round(dpr, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the differences for the sklearn metrics by using the difference_from_summary function. Let's illustrate this with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import difference_from_summary\n",
    "\n",
    "sum_acc = group_summary(accuracy_score, target, prediction, sensitive_features = gender)\n",
    "accd = difference_from_summary(sum_acc)\n",
    "print(\"Difference in accuracy: {}\".format(round(accd, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we have to do this for every metric, this is not user-friendly. Luckily, fairlean has an **interactive dashboard** that you can use to assess the fairness. This metric is part of *fairlearn.widget* and can be called using the *FairlearnDashboard* method. Let's illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73be9e4520784ef488b6f3ef89eada3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x1ea16e138e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "FairlearnDashboard(y_true = target,\n",
    "                   y_pred = prediction,\n",
    "                   sensitive_features = gender,\n",
    "                   sensitive_feature_names = [\"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same but this time for the race protected variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4596047574342d7ab0c5fd94c42f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x1ea1adeca60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FairlearnDashboard(y_true = target,\n",
    "                   y_pred = prediction,\n",
    "                   sensitive_features = race,\n",
    "                   sensitive_feature_names = [\"race\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigating unfairness\n",
    "In the previous section, we have identified some problems with the algorithm. Let's focus now on trying to solve the issues with the gender unfairness. Related to this gender unfairness we have identified that:\n",
    "- Men that should get a loan are disadvantage compared to women that should get a loan\n",
    "- Women in general are disadvantaged for getting a loan\n",
    "\n",
    "We can try to solve these issues by using build in fairness mitigation methods. In general, we use one of the *Reduction* method. These reduction methods requires three parameters:\n",
    "1. Base_estimater: the estimator that was used (usually comes from sklearn). In our case we used a DecisionTreeClassifier.\n",
    "2. Constraints: the fairness constraints that should be satisfied by the model. In fairlearn we have (for binary classification) the *DemographicParity*, *TruePositiveRateParity*, *EqualizedOdds* and the *ErrorRateParity* constraint.\n",
    "3. Sensitive features: which sensitive feature you want to take into account (in the fit method)\n",
    "\n",
    "The two reduction methods are: ExponentiatedGradient or GridSearch.\n",
    "\n",
    "Let's try to implement this on the gender unfairness. Since we are interested in getting the disparity in predictions right, we will start by implementing the demographic parity constraint. Then, we will try to solve the other unfairness by implementing the equalized odds constraint.\n",
    "\n",
    "##### DemographicParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
    "dp = DemographicParity()\n",
    "reduction = ExponentiatedGradient(classifier, dp)\n",
    "\n",
    "reduction.fit(df, target, sensitive_features=gender)\n",
    "prediction_dp = reduction.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EqualizedOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo = EqualizedOdds()\n",
    "reduction = ExponentiatedGradient(classifier, eo)\n",
    "\n",
    "reduction.fit(df, target, sensitive_features=gender)\n",
    "prediction_eo = reduction.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad32881f69b4026bf1eac9ca4456b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x1ea1b11df10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FairlearnDashboard(y_true = target,\n",
    "                   y_pred = {\"prediction_original\" : prediction,\n",
    "                             \"prediction_dp\": prediction_dp,\n",
    "                             \"prediction_eo\": prediction_eo},\n",
    "                   sensitive_features = gender,\n",
    "                   sensitive_feature_names = [\"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice how little accuracy we are sacrificing in order to get rid of the disparity in predictions (almost completely gone)!**\n",
    "\n",
    "How do we implement this in practice? Instead of using the classifier to predict, as we did initially, we just use the reduction that is trained by using the demographic parity constraint to make the predictions. In the end, this means that we only need a couple of extra lines of code to create a fair ML model. We did of course have to spends more time on the exploration phase.\n",
    "\n",
    "### Postprocessing technique\n",
    "\n",
    "We can also use the postprocessing package of fairlearn. More specifically the *ThresholdOptimizer*. This optimizer determines the optimal threshold using a trade-off between an objective (e.g. optimizing accuracy) and a constraint (e.g. the DemographicParity constraint).\n",
    "\n",
    "This ThresholdOptimizer only requires an estimator. By default it has the objective to optimize for accuracy score and the default constraint is DemographicParity. How to tweak these parameters can be found [here](https://fairlearn.github.io/api_reference/fairlearn.postprocessing.html). Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "reduction = ThresholdOptimizer(estimator = classifier)\n",
    "reduction.fit(df, target, sensitive_features = gender)\n",
    "prediction_threshold = reduction.predict(df, sensitive_features = gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d5dcf6c688446baac4d417e82f405f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x1ea1b450550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FairlearnDashboard(y_true = target,\n",
    "                   y_pred = {\"prediction_original\" : prediction,\n",
    "                             \"prediction_dp\": prediction_dp,\n",
    "                             \"prediction_eo\": prediction_eo,\n",
    "                             \"prediction_threshold\": prediction_threshold},\n",
    "                   sensitive_features = gender,\n",
    "                   sensitive_feature_names = [\"gender\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
